<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2N7Z90R9G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-J2N7Z90R9G');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bachelor Thesis - Hand Detection and Gesture Recognition</title>
    <link rel="icon" href="../assets/favicon.png" type="image/x-icon">
    <link rel="stylesheet" href="../css/projects.css">
</head>
<body>
    <div id="back-button">
        <button onclick="goBack()">← Back to Projects</button>
    </div>

    <main id="project-content">
        <section id="project">
            <div class="project-title">
                <h1>Hand Detection and Gesture Recognition</h1>
            </div>

            <div class="description-text">
                <p>
                    This project was developed as part of my Bachelor's thesis and focuses on two core components: automated data generation and training a deep learning model for hand detection and gesture recognition using TensorFlow in Python.
                </p>

                <h2>Data Generation</h2>
                <p>
                    To build a robust dataset, I used Unity along with a commercially available 3D hand model. I modified the model to enable human-like finger movements and constructed a dynamic scene setup with customizable lighting, backgrounds, and camera parameters.
                </p>
                <p>
                    I implemented a fully automated script that randomly adjusted these scene parameters and captured screenshots. This approach allowed me to create large, diverse datasets in minutes. Each parameter (e.g., background, light intensity, camera angle) could be toggled to study its effect on model performance.
                </p>

                <h2>Model Architecture and Training</h2>
                <p>
                    For each of the following tasks, I trained a separate model:
                    <ul>
                        <li><strong>Binary Classification:</strong> Determine whether a hand is present in the image.</li>
                        <li><strong>Gesture Recognition:</strong> Identify one of five hand poses.</li>
                    </ul>
                </p>
                <p>
                    I experimented with multiple model architectures, training strategies, and image augmentations. This included testing which alterations (e.g., color jitter, noise) improved or reduced accuracy.
                </p>
                <p>
                    For real-world validation, I created a test set of 100 images featuring my own hand in various locations around Vienna. The final models performed well, achieving 98% accuracy in the hand detection and up to 90% accuracy in the pose recognition task.
                </p>

                <p>
                    <a href="https://www.youtube.com/watch?v=3fsEKxG2vEQ" target="_blank">
                        ▶ Watch the YouTube video about the project
                    </a>
                </p>

                <p>
                    <a href="https://github.com/mani1709/BachelorThesisImageGeneration" target="_blank">
                        Link to the image generation repository of the project
                    </a>
                </p>

                <p>
                    <a href="https://github.com/mani1709/BachelorThesisImageClassification" target="_blank">
                        Link to the deep learning repository of the project
                    </a>
                </p>
            </div>
        </section>
    </main>
    <script src="../js/go_back.js"></script>
</body>
</html>
